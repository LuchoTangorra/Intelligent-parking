{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "os.path.expanduser('~')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json_data(url):\n",
    "    '''\n",
    "    Process and save the necessary information from the\n",
    "    json contained in the url (needs to be a gpssumo.com json)\n",
    "    \n",
    "    Parameters:\n",
    "        url: needs to be http://gpssumo.com/parquimetros/get_PA/ + some index, \n",
    "            e.g. id_cuadra\n",
    "    \n",
    "    Returns:\n",
    "        Same as process_json_data\n",
    "    '''\n",
    "    #Obtain the acces to the html\n",
    "    html = urllib.request.urlopen('http://gpssumo.com/parquimetros/get_PA/id_cuadra')\n",
    "    #Extract the text from the html\n",
    "    plain_data = BeautifulSoup(html, 'lxml').text\n",
    "\n",
    "    #Replace all the ' for \" cause the json cant have '\n",
    "    jsoned_data = plain_data.replace('\\'',\"\\\"\")\n",
    "    #Convert string data to json \n",
    "    json_data = json.loads(jsoned_data)\n",
    "    \n",
    "    process_json_data(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_json_data(json_data):\n",
    "    '''\n",
    "    Process the necessary information from the\n",
    "    json data.\n",
    "    It uses the last_data for all rows and the\n",
    "    filtered data, because we dont want repeated\n",
    "    information.\n",
    "    \n",
    "    Parameters:\n",
    "        json_data: json data from http://gpssumo.com/parquimetros/get_PA/ + some index, \n",
    "            e.g. id_cuadra\n",
    "    \n",
    "    Returns:\n",
    "        Same as process_json_data\n",
    "    '''\n",
    "    \n",
    "    #Cause we want those global variables to modify them and\n",
    "    #actualize them all the time\n",
    "    global last_data, filtered_data\n",
    "    \n",
    "    #This will be processing repeateadly\n",
    "    for i, data in enumerate(json_data):        \n",
    "        #Getting the possible new data\n",
    "        possible_new_data = {'id_cuadra': data['id_cuadra'], \n",
    "                             'direccion': data['direccion'],\n",
    "                             'fecha': data['fecha_a'],\n",
    "                             'tiempo': data['hora_a'],\n",
    "                             'ocupacion': data['ocupacion'],\n",
    "                             'lugares_cuadra': data['lugares_cuadra'],\n",
    "                             'ocupacion_max': data['ocupacion_max'],\n",
    "                             'dispon_parq': data['color'],\n",
    "                             'altas_bajas(dia)': data['trans_prk_dia']}\n",
    "\n",
    "        #Getting the id_cuadra, only for comparison reasons\n",
    "        id_cuadra = str(possible_new_data['id_cuadra'])\n",
    "\n",
    "        #obtaining the last data (ocupacion) of current id_cuadra\n",
    "        actual_last_data = last_data.loc[last_data.id_cuadra == id_cuadra, 'ocupacion']\n",
    "\n",
    "        append_data = False\n",
    "        #if the last data for the current id_cuadra is empty then we must append\n",
    "        if not actual_last_data.empty:\n",
    "            #if both have different value we must append\n",
    "            if not (actual_last_data == possible_new_data['ocupacion']).any():\n",
    "                append_data = True\n",
    "        else:\n",
    "            append_data = True\n",
    "\n",
    "        if append_data:\n",
    "            #overwriting the existing (or not) value for id_cuadra from last_data\n",
    "            last_data = last_data[last_data.id_cuadra != id_cuadra]\n",
    "            last_data = last_data.append(possible_new_data, ignore_index=True)\n",
    "\n",
    "            #Here we need to put the possible new data to a file instead of append it to filtered_data\n",
    "            filtered_data = filtered_data.append(possible_new_data, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'http://gpssumo.com/parquimetros/get_PA/id_cuadra'\n",
    "out_path = r'SUMO_data.csv'\n",
    "\n",
    "os.chdir(\"/home/varun/temp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    id_cuadra           direccion     fecha    tiempo  ocupacion  \\\n",
      "0           2   General Pinto 545  11-09-19  15:15:42          6   \n",
      "1           3      San Martín 452  11-09-19  15:13:40          6   \n",
      "2           4       Rodriguez 552  11-09-19  15:12:28         11   \n",
      "3           5      9 de Julio 441  11-09-19  15:10:32          8   \n",
      "4           6   General Pinto 759  11-09-19  15:14:10         14   \n",
      "5           7      San Martín 758  11-09-19  15:11:12          3   \n",
      "6           8       Sarmiento 641  11-09-19  15:10:16          4   \n",
      "7           9      San Martín 560  11-09-19  15:10:44          3   \n",
      "8          10       Rodriguez 654  11-09-19  15:13:50          4   \n",
      "9          11      San Martín 658  11-09-19  15:15:30          7   \n",
      "10         12        Belgrano 480  11-09-19  15:10:41          2   \n",
      "11         13   General Pinto 343  11-09-19  15:11:24          1   \n",
      "12         14       Rodriguez 464  11-09-19  15:12:17          7   \n",
      "13         15   General Pinto 631  11-09-19  15:11:02         10   \n",
      "14         16      San Martín 852  11-09-19  15:13:24          2   \n",
      "15         17        Belgrano 660  11-09-19  15:10:22          7   \n",
      "16         19       Sarmiento 543  11-09-19  15:14:24          7   \n",
      "17         20   General Pinto 490  11-09-19  15:10:32          7   \n",
      "18         21        Belgrano 540  11-09-19  15:15:20          8   \n",
      "19         22        Belgrano 768  11-09-19  15:10:57          7   \n",
      "20         23        Belgrano 364  11-09-19  15:10:29          2   \n",
      "21         24        Belgrano 868  11-09-19  15:11:30          4   \n",
      "22         25        Belgrano 958  11-09-19  15:10:23          0   \n",
      "23         26      9 de Julio 345  11-09-19  15:10:30          4   \n",
      "24         27   General Pinto 843  11-09-19  15:10:26          4   \n",
      "25         28   General Pinto 945  11-09-19  15:10:23          4   \n",
      "26         29      San Martín 364  11-09-19  15:10:35          3   \n",
      "27         30      San Martín 944  11-09-19  15:10:25          4   \n",
      "28         31       Sarmiento 351  11-09-19  15:10:29          4   \n",
      "29         32       Sarmiento 453  11-09-19  15:15:14         11   \n",
      "..        ...                 ...       ...       ...        ...   \n",
      "67         70     14 de Julio 767  11-09-19  15:10:35          0   \n",
      "68         71     14 de Julio 847  11-09-19  15:10:28          2   \n",
      "69         72           Maipu 453  11-09-19  15:10:22          2   \n",
      "70         73           Maipu 547  11-09-19  15:10:28          4   \n",
      "71         74           Maipu 653  11-09-19  15:12:11          3   \n",
      "72         75           Maipu 333  11-09-19  15:12:59          6   \n",
      "73         76           Maipu 741  11-09-19  15:12:08          6   \n",
      "74         77           Maipu 857  11-09-19  15:10:24          0   \n",
      "75         78           Maipu 953  11-09-19  15:10:22          1   \n",
      "76         79     General Paz 347  11-09-19  15:10:16          1   \n",
      "77         80     General Paz 447  11-09-19  15:10:57          4   \n",
      "78         81     General Paz 547  11-09-19  15:10:15          2   \n",
      "79         82     General Paz 647  11-09-19  15:13:12          5   \n",
      "80         83     General Paz 739  11-09-19  15:10:30          6   \n",
      "81         84     General Paz 845  11-09-19  15:10:27          2   \n",
      "82         85      Av. España 359  11-09-19  15:11:04          1   \n",
      "83         86      Av. España 481  11-09-19  15:10:39          1   \n",
      "84         87      Av. España 545  11-09-19  15:10:22          0   \n",
      "85         88      Av. España 647  11-09-19  15:10:37          0   \n",
      "86         89      Av. España 737  11-09-19  15:10:40          5   \n",
      "87         90      Av. España 843  11-09-19  15:10:36          0   \n",
      "88         91      Av. España 957  11-09-19  15:10:06          1   \n",
      "89         92  Tribunal de Faltas  11-09-19  15:10:27          0   \n",
      "90         93      Av. España 352  11-09-19  15:10:23          0   \n",
      "91         94      Av. España 436  11-09-19  15:10:28          2   \n",
      "92         95      Av. España 526  11-09-19  15:10:31          0   \n",
      "93         96      Av. España 624  11-09-19  15:10:31          3   \n",
      "94         97      Av. España 774  11-09-19  15:10:26          3   \n",
      "95         98      Av. España 864  11-09-19  15:10:35          1   \n",
      "96         99      Av. España 970  11-09-19  15:10:10          3   \n",
      "\n",
      "    lugares_cuadra  ocupacion_max dispon_parq  altas_bajas(dia)  \n",
      "0             27.0             17       green               148  \n",
      "1             19.0             11       green                84  \n",
      "2             28.0             16       green               135  \n",
      "3             26.0             14       green               183  \n",
      "4             30.0             18       green               167  \n",
      "5             28.0              9       green               104  \n",
      "6             22.0             11       green                95  \n",
      "7             22.0             10       green                44  \n",
      "8             24.0             12       green                96  \n",
      "9             30.0             15       green               153  \n",
      "10            25.0             16       green               128  \n",
      "11            27.0             22       green               134  \n",
      "12            30.0             10       green               124  \n",
      "13            25.0             15       green               117  \n",
      "14            32.0             10       green                94  \n",
      "15            23.0             17       green               129  \n",
      "16            22.0             13       green               141  \n",
      "17            24.0             14       green                87  \n",
      "18            29.0             21       green               158  \n",
      "19            35.0             13       green               159  \n",
      "20            17.0              6       green                31  \n",
      "21            18.0              9       green                61  \n",
      "22            13.0              3       green                25  \n",
      "23            20.0             12       green                87  \n",
      "24            22.0             11       green               115  \n",
      "25            12.0              7       green                58  \n",
      "26            19.0              7       green                79  \n",
      "27            23.0             18       green                84  \n",
      "28            18.0             13       green                89  \n",
      "29            27.0             15       green                94  \n",
      "..             ...            ...         ...               ...  \n",
      "67            19.0              6       green                26  \n",
      "68            20.0              2       green                12  \n",
      "69            33.0             10       green                82  \n",
      "70            21.0             11       green                50  \n",
      "71            20.0             12       green                86  \n",
      "72            31.0             12       green               137  \n",
      "73            24.0             13       green               121  \n",
      "74            15.0              1       green                 5  \n",
      "75            14.0              4       green                26  \n",
      "76            11.0              3       green                31  \n",
      "77            36.0             17       green               150  \n",
      "78            17.0              7       green                45  \n",
      "79            34.0             14       green                93  \n",
      "80            18.0              9       green                73  \n",
      "81            18.0              5       green                42  \n",
      "82             7.0              4       green                22  \n",
      "83            10.0              3       green                21  \n",
      "84            10.0              2       green                15  \n",
      "85            11.0              4       green                35  \n",
      "86            19.0             10       green                90  \n",
      "87            15.0              4       green                13  \n",
      "88            13.0              5       green                34  \n",
      "89             NaN              0       green                 2  \n",
      "90             NaN              4       green                24  \n",
      "91             NaN              3       green                15  \n",
      "92             NaN              4       green                29  \n",
      "93             NaN              4       green                36  \n",
      "94             NaN              6       green                43  \n",
      "95             NaN              6       green                32  \n",
      "96             NaN              4       green                47  \n",
      "\n",
      "[97 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "#Initialize the variables used by the scraper\n",
    "#filtered_data is the output data\n",
    "filtered_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "\n",
    "#the new readed data\n",
    "possible_new_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "\n",
    "#last_data is the last data apended to filtered_data, this is used to have\n",
    "#only one copy of the data in filtered_data\n",
    "last_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "if os.path.isfile(out_path): # if file does exist get the data from the csv file \n",
    "    csv_data = pd.read_csv(out_path, delimiter=',')\n",
    "    last_data = csv_data.groupby('id_cuadra').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "paso 0 min\n",
      "paso 1 min\n",
      "paso 2 min\n",
      "paso 3 min\n",
      "paso 4 min\n",
      "paso 5 min\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'SUMO_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-eede25313a4a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m             \u001b[0mfiltered_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# else it exists so append without writing the header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m             \u001b[0mfiltered_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m             \u001b[0mfiltered_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfiltered_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, tupleize_cols, date_format, doublequote, escapechar, decimal)\u001b[0m\n\u001b[0;32m   3018\u001b[0m                                  \u001b[0mdoublequote\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdoublequote\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3019\u001b[0m                                  escapechar=escapechar, decimal=decimal)\n\u001b[1;32m-> 3020\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3021\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3022\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    155\u001b[0m             f, handles = _get_handle(self.path_or_buf, self.mode,\n\u001b[0;32m    156\u001b[0m                                      \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 157\u001b[1;33m                                      compression=self.compression)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36m_get_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text)\u001b[0m\n\u001b[0;32m    422\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;31m# Python 3 and encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 424\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    425\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m             \u001b[1;31m# Python 3 and no explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'SUMO_data.csv'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    #get the actual time to sleep only 1 minute    \n",
    "    starttime = time.time()\n",
    "    \n",
    "    #append the new data to the filtered_data var we want to write in the file\n",
    "    save_json_data(url)\n",
    "\n",
    "    if filtered_data is not None:\n",
    "        if not os.path.isfile(out_path): # if file does not exist write header \n",
    "            filtered_data.to_csv(out_path, index=False, header=filtered_data.columns.values)\n",
    "            filtered_data = filtered_data[0:0]\n",
    "        else: # else it exists so append without writing the header\n",
    "            filtered_data.to_csv(out_path, index=False, mode='a', header=False)\n",
    "            filtered_data = filtered_data[0:0]\n",
    "    \n",
    "    #sleep for 1 minute\n",
    "    time.sleep(60.0 - ((time.time() - starttime) % 60.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
