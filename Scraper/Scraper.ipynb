{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:15:40.626688Z",
     "start_time": "2020-03-18T13:15:38.966855Z"
    }
   },
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:15:40.638889Z",
     "start_time": "2020-03-18T13:15:40.629836Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_json_data(url):\n",
    "    '''\n",
    "    Process and save the necessary information from the\n",
    "    json contained in the url (needs to be a gpssumo.com json)\n",
    "    \n",
    "    Parameters:\n",
    "        url: needs to be http://gpssumo.com/parquimetros/get_PA/ + some index, \n",
    "            e.g. id_cuadra\n",
    "    \n",
    "    Returns:\n",
    "        Same as process_json_data\n",
    "    '''\n",
    "    #Obtain the acces to the html\n",
    "    html = urllib.request.urlopen('http://gpssumo.com/parquimetros/get_PA/id_cuadra')\n",
    "    #Extract the text from the html\n",
    "    plain_data = BeautifulSoup(html, 'lxml').text\n",
    "\n",
    "    #Replace all the ' for \" cause the json cant have '\n",
    "    jsoned_data = plain_data.replace('\\'',\"\\\"\")\n",
    "    #Convert string data to json \n",
    "    json_data = json.loads(jsoned_data)\n",
    "    \n",
    "    process_json_data(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:15:40.654834Z",
     "start_time": "2020-03-18T13:15:40.643076Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def process_json_data(json_data):\n",
    "    '''\n",
    "    Process the necessary information from the\n",
    "    json data.\n",
    "    It uses the last_data for all rows and the\n",
    "    filtered data, because we dont want repeated\n",
    "    information.\n",
    "    \n",
    "    Parameters:\n",
    "        json_data: json data from http://gpssumo.com/parquimetros/get_PA/ + some index, \n",
    "            e.g. id_cuadra\n",
    "    \n",
    "    Returns:\n",
    "        Same as process_json_data\n",
    "    '''\n",
    "    \n",
    "    #Cause we want those global variables to modify them and\n",
    "    #actualize them all the time\n",
    "    global last_data, filtered_data\n",
    "    \n",
    "    #This will be processing repeateadly\n",
    "    for i, data in enumerate(json_data):        \n",
    "        #Getting the possible new data\n",
    "        possible_new_data = {'id_cuadra': data['id_cuadra'], \n",
    "                             'direccion': data['direccion'],\n",
    "                             'fecha': data['fecha_a'],\n",
    "                             'tiempo': data['hora_a'],\n",
    "                             'ocupacion': data['ocupacion'],\n",
    "                             'lugares_cuadra': data['lugares_cuadra'],\n",
    "                             'ocupacion_max': data['ocupacion_max'],\n",
    "                             'dispon_parq': data['color'],\n",
    "                             'altas_bajas(dia)': data['trans_prk_dia']}\n",
    "\n",
    "        #Getting the id_cuadra, only for comparison reasons\n",
    "        id_cuadra = str(possible_new_data['id_cuadra'])\n",
    "\n",
    "        #obtaining the last data (ocupacion) of current id_cuadra\n",
    "        actual_last_data = last_data.loc[last_data.id_cuadra == id_cuadra, 'ocupacion']\n",
    "\n",
    "        append_data = False\n",
    "        #if the last data for the current id_cuadra is empty then we must append\n",
    "        if not actual_last_data.empty:\n",
    "            #if both have different value we must append\n",
    "            if not (actual_last_data == possible_new_data['ocupacion']).any():\n",
    "                append_data = True\n",
    "        else:\n",
    "            append_data = True\n",
    "\n",
    "        if append_data:\n",
    "            #overwriting the existing (or not) value for id_cuadra from last_data\n",
    "            last_data = last_data[last_data.id_cuadra != id_cuadra]\n",
    "            last_data = last_data.append(possible_new_data, ignore_index=True)\n",
    "\n",
    "            #Here we need to put the possible new data to a file instead of append it to filtered_data\n",
    "            filtered_data = filtered_data.append(possible_new_data, ignore_index=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:15:40.666896Z",
     "start_time": "2020-03-18T13:15:40.656828Z"
    }
   },
   "outputs": [],
   "source": [
    "url = 'http://gpssumo.com/parquimetros/get_PA/id_cuadra'\n",
    "out_path = r'SUMO_data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:15:40.699316Z",
     "start_time": "2020-03-18T13:15:40.669431Z"
    }
   },
   "outputs": [],
   "source": [
    "#Initialize the variables used by the scraper\n",
    "#filtered_data is the output data\n",
    "filtered_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "\n",
    "#the new readed data\n",
    "possible_new_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "\n",
    "#last_data is the last data apended to filtered_data, this is used to have\n",
    "#only one copy of the data in filtered_data\n",
    "last_data = pd.DataFrame(columns=['id_cuadra','direccion','fecha','tiempo','ocupacion','lugares_cuadra','ocupacion_max','dispon_parq','altas_bajas(dia)'])\n",
    "if os.path.isfile(out_path): # if file does exist get the data from the csv file \n",
    "    csv_data = pd.read_csv(out_path, delimiter=',')\n",
    "    last_data = csv_data.groupby('id_cuadra').last().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-18T13:16:40.843241Z",
     "start_time": "2020-03-18T13:15:40.700836Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:  0\n",
      "Trying to save some data...\n",
      "Data saved succesfully!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7a2f2b0550a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;31m#sleep for 1 minute\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m60.0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstarttime\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m60.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "internet_on = True\n",
    "while True:\n",
    "    #get the actual time to sleep only 1 minute    \n",
    "    starttime = time.time()\n",
    "    \n",
    "    print('Iteration: ', i)\n",
    "    i+=1\n",
    "    #try/catch to avoid no internet conection or any other problem\n",
    "    try:\n",
    "        #append the new data to the filtered_data var we want to write in the file\n",
    "        print('Trying to save some data...')\n",
    "        save_json_data(url)\n",
    "        if filtered_data is not None:\n",
    "            if not os.path.isfile(out_path): # if file does not exist write header \n",
    "                filtered_data.to_csv(out_path, index=False, header=filtered_data.columns.values)\n",
    "                filtered_data = filtered_data[0:0]\n",
    "            else: # else it exists so append without writing the header\n",
    "                filtered_data.to_csv(out_path, index=False, mode='a', header=False)\n",
    "                filtered_data = filtered_data[0:0]\n",
    "        print('Data saved succesfully!')\n",
    "        \n",
    "    except:\n",
    "        #If we have no internet conection (or any other problem) then sleep the process for one minute\n",
    "        print('Having some kind of problem (maybe with the internet), huh? Ill sleep, cya')\n",
    "        \n",
    "    #sleep for 1 minute\n",
    "    time.sleep(60.0 - ((time.time() - starttime) % 60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
